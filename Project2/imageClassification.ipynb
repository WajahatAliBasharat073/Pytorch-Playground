{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9fc1512a",
   "metadata": {},
   "source": [
    "### About This Data\n",
    "This dataset, also known as Animal Faces-HQ (AFHQ), consists of 16,130 high-quality images at 512Ã—512 resolution.\n",
    "There are three domains of classes, each providing about 5000 images. By having multiple (three) domains and diverse images of various breeds per each domain, AFHQ sets a challenging image-to-image translation problem. The classes are:\n",
    "\n",
    "- Cat;\n",
    "- Dog;\n",
    "- Wildlife."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebbe4d12",
   "metadata": {},
   "source": [
    "## Libraries import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from PIL import Image ##image reading\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchsummary import summary\n",
    "from torchvision import transforms ## preprocessing images\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, OneHotEncoder\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## transfer from cpu to gpu\n",
    "device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4218fd0",
   "metadata": {},
   "source": [
    "## Data Loading\n",
    "- read all dataset so we can later split it into train, val, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = []\n",
    "labels = []\n",
    "base_dir = \"E:/AI 2025/Projects_Pytorch/Datasets/AnimalFaces\"\n",
    "\n",
    "for i in os.listdir(base_dir):\n",
    "    # print(i)\n",
    "    for label in os.listdir(f\"{base_dir}/{i}\"):\n",
    "        # print(label)\n",
    "        for image in os.listdir(f\"{base_dir}/{i}/{label}\"):\n",
    "            # print(image)\n",
    "            image_path.append(f\"{base_dir}/{i}/{label}/{image}\")\n",
    "            labels.append(label)\n",
    "df=pd.DataFrame(zip(image_path,labels), columns=[\"image_path\",\"labels\"])\n",
    "print(df['labels'].value_counts())\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e496018",
   "metadata": {},
   "source": [
    "## Split data into train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = df.sample(frac=0.7, random_state=42)\n",
    "test = df.drop(train.index)\n",
    "val = test.sample(frac=0.5, random_state=42)\n",
    "test = test.drop(val.index)\n",
    "print(f\"Train: {len(train)}, Test: {len(test)}, Val: {len(val)}\")\n",
    "print(train.shape, test.shape, val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Use label encoder to convert labels to numbers\n",
    "labelEncoder = LabelEncoder()\n",
    "labelEncoder.fit(df['labels'])\n",
    "## make all images have same properties\n",
    "transforms = transforms.Compose([\n",
    "    transforms.Resize((128,128)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.ConvertImageDtype(torch.float32)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3328d56d",
   "metadata": {},
   "source": [
    "## Create Custom Image dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AnimalFacesDataset(Dataset):\n",
    "    def __init__(self, df, transform=None):\n",
    "        self.df = df\n",
    "        self.transform = transform\n",
    "        self.labels = torch.tensor(labelEncoder.transform(df['labels'])).to(device)\n",
    "    def __len__(self):\n",
    "        return self.df.shape[0]\n",
    "    def __getitem__(self, idx):\n",
    "        image_path = self.df.iloc[idx,0]\n",
    "        label = self.labels[idx]\n",
    "        image=Image.open(image_path).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            image=self.transform(image).to(device)\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = AnimalFacesDataset(df=train, transform=transforms)\n",
    "val_dataset = AnimalFacesDataset(df=val, transform=transforms)\n",
    "test_dataset = AnimalFacesDataset(df=test, transform=transforms)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "373a5eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Visulize some images\n",
    "n_rows=3\n",
    "n_col=3\n",
    "f,ax=plt.subplots(n_rows,n_col,figsize=(10,10))\n",
    "for i in range(n_rows):\n",
    "    for j in range(n_col):\n",
    "        image=Image.open(df.sample(n=1)['image_path'].values[0]).convert(\"RGB\")\n",
    "        ax[i,j].imshow(image)\n",
    "        ax[i,j].axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR=0.001\n",
    "BATCH_SIZE=32\n",
    "EPOCHS=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_Dataloader=DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_Dataloader=DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_Dataloader=DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55fb2b3d",
   "metadata": {},
   "source": [
    "### Custom Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy._core.fromnumeric import searchsorted\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1=nn.Conv2d(3,32,kernel_size=3,padding=1)\n",
    "        self.conv2=nn.Conv2d(32,64,kernel_size=3,padding=1)\n",
    "        self.conv3=nn.Conv2d(64,128,kernel_size=3,padding=1)\n",
    "\n",
    "\n",
    "        self.pooling=nn.MaxPool2d(kernel_size=2,stride=2)\n",
    "        self.relu=nn.ReLU()\n",
    "        self.flatten=nn.Flatten()\n",
    "        self.linear=nn.Linear((128*16*16),128)\n",
    "        self.output=nn.Linear(128,len(df['labels'].unique()))\n",
    "    def forward(self,x):\n",
    "        x=self.conv1(x)\n",
    "        x=self.pooling(x)\n",
    "        x=self.relu(x)\n",
    "\n",
    "        x=self.conv2(x)\n",
    "        x=self.pooling(x) \n",
    "        x=self.relu(x)\n",
    "\n",
    "        x=self.conv3(x)\n",
    "        x=self.pooling(x)\n",
    "        x=self.relu(x)\n",
    "\n",
    "        x=self.flatten(x)\n",
    "        x=self.linear(x)\n",
    "        x=self.output(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=Net().to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchsummary import summary\n",
    "summary(model,input_size=(3,128,128))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion=nn.CrossEntropyLoss()\n",
    "optimizer=Adam(model.parameters(),lr=LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_loss_train_plot=[]\n",
    "total_loss_val_plot=[]\n",
    "total_acc_train_plot=[]\n",
    "total_acc_val_plot=[]\n",
    "\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "\n",
    "\n",
    "    total_loss_train=0\n",
    "    total_acc_train=0\n",
    "    total_loss_val=0\n",
    "    total_acc_val=0\n",
    "\n",
    "\n",
    "    for images,labels in train_Dataloader:\n",
    "\n",
    "        images,labels=images.to(device),labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs=model(images)\n",
    "        train_loss=criterion(outputs,labels)\n",
    "        total_loss_train+=train_loss.item()\n",
    "        train_loss.backward()\n",
    "        train_acc=(torch.argmax(outputs, dim=1)==labels).sum().item()\n",
    "        total_acc_train+=train_acc\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images,labels in val_Dataloader:\n",
    "            output=model(images)\n",
    "            val_loss=criterion(output,labels)\n",
    "            total_loss_val+=val_loss.item()\n",
    "            val_acc=(torch.argmax(output, dim=1)==labels).sum().item()\n",
    "            total_acc_val+=val_acc\n",
    "\n",
    "\n",
    "    total_loss_train_plot.append(round(total_loss_train/1000,4))\n",
    "    total_loss_val_plot.append(round(total_loss_val/1000,4))\n",
    "    total_acc_train_plot.append(round((total_acc_train/train_dataset.__len__())*100,4))\n",
    "    total_acc_val_plot.append(round((total_acc_val/val_dataset.__len__())*100,4))\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS}, Training Loss: {total_loss_train_plot[-1]:.4f}, Training Accuracy: {total_acc_train_plot[-1]:.4f}, Validation Loss: {total_loss_val_plot[-1]:.4f}, Validation Accuracy: {total_acc_val_plot[-1]:.4f}\")\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(total_loss_train_plot,label=\"Train Loss\")\n",
    "plt.plot(total_loss_val_plot,label=\"Validation Loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    total_loss_test = 0\n",
    "    total_accuracy_test = 0\n",
    "\n",
    "    for data in test_Dataloader:\n",
    "        input,label=data\n",
    "        predictions = model(input).squeeze(1)\n",
    "        batch_loss_test = criterion(predictions, label).item()\n",
    "        total_loss_test += batch_loss_test\n",
    "        total_accuracy_test += (predictions.round() == label).sum().item()\n",
    "print(\"Accuracy of the model is: \",round(total_accuracy_test/test_dataset.__len__()*100,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig ,axis=plt.subplots(nrows=1,ncols=2,figsize=(10,5))\n",
    "axis[0].plot(total_loss_train_plot,label=\"Training Loss\")\n",
    "axis[0].plot(total_loss_val_plot,label=\"Validation Loss\")\n",
    "axis[0].set_title(\"Loss\")\n",
    "axis[0].legend()\n",
    "axis[1].plot(total_acc_train_plot,label=\"Training Accuracy\")\n",
    "axis[1].plot(total_acc_val_plot,label=\"Validation Accuracy\")\n",
    "axis[1].set_title(\"Accuracy\")\n",
    "axis[1].legend()\n",
    "plt.show()\n",
    "\n",
    "axis[1].plot(total_acc_train_plot,label=\"Training Accuracy\")\n",
    "axis[1].plot(total_acc_val_plot,label=\"Validation Accuracy\")\n",
    "axis[1].set_title(\"Accuracy\")\n",
    "axis[1].legend()\n",
    "axis[1].set_xlabel(\"Epochs\")\n",
    "axis[1].set_ylabel(\"Accuracy\")\n",
    "axis[1].grid(True)\n",
    "axis[1].set_ylim(0,100)\n",
    "axis[1].set_xlim(0,EPOCHS)\n",
    "axis[1].set_xticks(np.arange(0,EPOCHS+1,1))\n",
    "axis[1].set_yticks(np.arange(0,101,10))\n",
    "axis[1].set_xticklabels(np.arange(0,EPOCHS+1,1))\n",
    "axis[1].set_yticklabels(np.arange(0,101,10))\n",
    "axis[1].set_xticklabels(np.arange(0,EPOCHS+1,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "701b0c74",
   "metadata": {},
   "source": [
    "### Perform inference\n",
    "- read image\n",
    "- Transform using transform object\n",
    "- predict the model\n",
    "- inverse transform by label encoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(image_path):\n",
    "    image=Image.open(image_path).convert(\"RGB\")\n",
    "    image=transforms(image).to(device)\n",
    "    print(image.shape)\n",
    "    output=model(image.unsqueeze(0))\n",
    "    return torch.argmax(output,dim=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict(\"E:/AI 2025/Projects_Pytorch/Project2/cat.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
