{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e92fbc55",
   "metadata": {},
   "source": [
    "### About Dataset\n",
    "The dataset contains photographs of three legume leaf lesion states, including 1034 images in the training set and 133 images in the calibration set, with a data size of 155 MB. the following leaf lesion states are supported to be recognized: healthy, angular_leaf_spot, and bean_rust."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8fd18876",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: opendatasets in e:\\ai 2025\\projects_pytorch\\venv\\lib\\site-packages (0.1.22)\n",
      "Requirement already satisfied: tqdm in e:\\ai 2025\\projects_pytorch\\venv\\lib\\site-packages (from opendatasets) (4.67.1)\n",
      "Requirement already satisfied: kaggle in e:\\ai 2025\\projects_pytorch\\venv\\lib\\site-packages (from opendatasets) (1.7.4.5)\n",
      "Requirement already satisfied: click in e:\\ai 2025\\projects_pytorch\\venv\\lib\\site-packages (from opendatasets) (8.2.1)\n",
      "Requirement already satisfied: colorama in e:\\ai 2025\\projects_pytorch\\venv\\lib\\site-packages (from click->opendatasets) (0.4.6)\n",
      "Requirement already satisfied: bleach in e:\\ai 2025\\projects_pytorch\\venv\\lib\\site-packages (from kaggle->opendatasets) (6.2.0)\n",
      "Requirement already satisfied: certifi>=14.05.14 in e:\\ai 2025\\projects_pytorch\\venv\\lib\\site-packages (from kaggle->opendatasets) (2025.7.14)\n",
      "Requirement already satisfied: charset-normalizer in e:\\ai 2025\\projects_pytorch\\venv\\lib\\site-packages (from kaggle->opendatasets) (3.4.2)\n",
      "Requirement already satisfied: idna in e:\\ai 2025\\projects_pytorch\\venv\\lib\\site-packages (from kaggle->opendatasets) (3.10)\n",
      "Requirement already satisfied: protobuf in e:\\ai 2025\\projects_pytorch\\venv\\lib\\site-packages (from kaggle->opendatasets) (6.31.1)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in e:\\ai 2025\\projects_pytorch\\venv\\lib\\site-packages (from kaggle->opendatasets) (2.9.0.post0)\n",
      "Requirement already satisfied: python-slugify in e:\\ai 2025\\projects_pytorch\\venv\\lib\\site-packages (from kaggle->opendatasets) (8.0.4)\n",
      "Requirement already satisfied: requests in e:\\ai 2025\\projects_pytorch\\venv\\lib\\site-packages (from kaggle->opendatasets) (2.32.4)\n",
      "Requirement already satisfied: setuptools>=21.0.0 in e:\\ai 2025\\projects_pytorch\\venv\\lib\\site-packages (from kaggle->opendatasets) (70.2.0)\n",
      "Requirement already satisfied: six>=1.10 in e:\\ai 2025\\projects_pytorch\\venv\\lib\\site-packages (from kaggle->opendatasets) (1.17.0)\n",
      "Requirement already satisfied: text-unidecode in e:\\ai 2025\\projects_pytorch\\venv\\lib\\site-packages (from kaggle->opendatasets) (1.3)\n",
      "Requirement already satisfied: urllib3>=1.15.1 in e:\\ai 2025\\projects_pytorch\\venv\\lib\\site-packages (from kaggle->opendatasets) (2.5.0)\n",
      "Requirement already satisfied: webencodings in e:\\ai 2025\\projects_pytorch\\venv\\lib\\site-packages (from kaggle->opendatasets) (0.5.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install opendatasets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import opendatasets as od\n",
    "# od.download(\"https://www.kaggle.com/datasets/marquis03/bean-leaf-lesions-classification\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3265ae2",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim import Adam\n",
    "from torchvision.transforms import transforms\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "from torchvision import models\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "from  PIL import Image\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# gpu check\n",
    "device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10e335db",
   "metadata": {},
   "source": [
    "## Data Loading & Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df=pd.read_csv(r\"E:\\AI 2025\\Projects_Pytorch\\Datasets\\Bean Leaf Classification\\train.csv\")\n",
    "val_df=pd.read_csv(r\"E:\\AI 2025\\Projects_Pytorch\\Datasets\\Bean Leaf Classification\\val.csv\")\n",
    "\n",
    "train_df[\"image:FILE\"] = \"E:/AI 2025/Projects_Pytorch/Datasets/Bean Leaf Classification/\" + train_df[\"image:FILE\"]\n",
    "val_df[\"image:FILE\"] = \"E:/AI 2025/Projects_Pytorch/Datasets/Bean Leaf Classification/\" + val_df[\"image:FILE\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image:FILE</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>E:/AI 2025/Projects_Pytorch/Datasets/Bean Leaf...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>E:/AI 2025/Projects_Pytorch/Datasets/Bean Leaf...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>E:/AI 2025/Projects_Pytorch/Datasets/Bean Leaf...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>E:/AI 2025/Projects_Pytorch/Datasets/Bean Leaf...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>E:/AI 2025/Projects_Pytorch/Datasets/Bean Leaf...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          image:FILE  category\n",
       "0  E:/AI 2025/Projects_Pytorch/Datasets/Bean Leaf...         0\n",
       "1  E:/AI 2025/Projects_Pytorch/Datasets/Bean Leaf...         0\n",
       "2  E:/AI 2025/Projects_Pytorch/Datasets/Bean Leaf...         0\n",
       "3  E:/AI 2025/Projects_Pytorch/Datasets/Bean Leaf...         0\n",
       "4  E:/AI 2025/Projects_Pytorch/Datasets/Bean Leaf...         0"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image:FILE</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>E:/AI 2025/Projects_Pytorch/Datasets/Bean Leaf...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>E:/AI 2025/Projects_Pytorch/Datasets/Bean Leaf...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>E:/AI 2025/Projects_Pytorch/Datasets/Bean Leaf...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>E:/AI 2025/Projects_Pytorch/Datasets/Bean Leaf...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>E:/AI 2025/Projects_Pytorch/Datasets/Bean Leaf...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          image:FILE  category\n",
       "0  E:/AI 2025/Projects_Pytorch/Datasets/Bean Leaf...         0\n",
       "1  E:/AI 2025/Projects_Pytorch/Datasets/Bean Leaf...         0\n",
       "2  E:/AI 2025/Projects_Pytorch/Datasets/Bean Leaf...         0\n",
       "3  E:/AI 2025/Projects_Pytorch/Datasets/Bean Leaf...         0\n",
       "4  E:/AI 2025/Projects_Pytorch/Datasets/Bean Leaf...         0"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df[\"category\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "category\n",
       "2    348\n",
       "1    345\n",
       "0    341\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df[\"category\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "## this help in preprocess the images all are same type and same size\n",
    "transform =  transforms.Compose([\n",
    "    transforms.Resize((128,128)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.ConvertImageDtype(torch.float64)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cProfile import label\n",
    "\n",
    "\n",
    "class customImageDataset(Dataset):\n",
    "    def __init__(self, dataframe,transform):\n",
    "        self.dataframe=dataframe\n",
    "        self.transform=transform\n",
    "        self.labels=torch.tensor(dataframe[\"category\"]).to(device)\n",
    "    def __len__(self):\n",
    "        return self.dataframe.shape[0]\n",
    "    def __getitem__(self, index):\n",
    "        img_path=self.dataframe.iloc[index,0]\n",
    "        label=self.labels[index]\n",
    "        image=Image.open(img_path)\n",
    "        if self.transform:\n",
    "            image=(self.transform(image)/255.0).to(device)\n",
    "        return image, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset=customImageDataset(dataframe=train_df, transform=transform)\n",
    "val_dataset=customImageDataset(dataframe=val_df,transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x1000 with 25 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "n_rows = 5\n",
    "n_cols = 5\n",
    "\n",
    "# Create subplots\n",
    "f, axarr = plt.subplots(n_rows, n_cols, figsize=(10, 10))\n",
    "\n",
    "for row in range(n_rows):\n",
    "    for col in range(n_cols):\n",
    "        idx = np.random.randint(0, len(train_dataset))\n",
    "        image, _ = train_dataset[idx]  # unpack image and label if dataset returns a tuple\n",
    "\n",
    "        if isinstance(image, torch.Tensor):\n",
    "            image = image.to(\"cpu\")  # make sure it's on CPU for plotting\n",
    "\n",
    "        image = (image * 255.0).squeeze().permute(1, 2, 0).numpy().astype(np.uint8)\n",
    "        axarr[row, col].imshow(image)\n",
    "        axarr[row, col].axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
